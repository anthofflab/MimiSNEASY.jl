################################################################################################
###Metropolosis algorithm
proposalfunction = function(param){
return(rnorm(3, mean=param, sd = c(0.1, 0.5, 0.3)))
}
run_metropolosis_MCMC = function(startvalue, iterations){
chain = array(dim = c(iterations +1, 3))
chain[1, ] = startvalue
for (i in 1:iterations){
proposal = proposalfunction(chain[i, ])
probab = exp(posterior(proposal) - posterior(chain[i , ]))
if (runif(1) < probab){
chain[i+1, ] = proposal
}else{
chain[i+1, ] = chain[i, ]
}
}
return(chain)
}
startvalue = c(4, 0 , 10)
chain = run_metropolosis_MCMC(startvalue, 100000)
burnIn = 5000
hist(chain[-(1:burnIn), 1], nclass= 50, main="Posterior of A")
abline(v = trueA, col="red")
#https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/
trueA = 12
trueB = 0
trueSd =15
sampleSize = 101
#create independent x-values
x = (-(sampleSize-1)/2):((sampleSize-1)/2)
#create y values as lineary combo plus error (ax + b + N(0,sd))
y = trueA * x + trueB + rnorm(n = sampleSize, mean= 0, sd = trueSd)
plot(x,y, main="Test Data")
#create liklihood function
#dnorm gives heights of probability distribution at each point with mean and sd specified
#use log here because prob hieghts can get very small, and might run into computer precision errors.
likelihood = function(param){
a = param[1]
b = param[2]
sd = param[3]
pred = a*x + b
singlelikelihoods = dnorm(y, mean = pred, sd = sd, log = TRUE)
sumll = sum(singlelikelihoods)
return(sumll)
}
#example plot to show that it estimates highest prob density over true a parameter if we give it the actual B and SD parameters.
slopevalues = function(potential_a){return(likelihood(c(potential_a, trueB, trueSd)))}
slopelikelihoods = lapply(seq(3,24, by=0.05), slopevalues)
plot (seq(3,24, by = 0.05), slopelikelihoods, type="l", xlab="value of slope paramater a", ylab = "Log likelihood")
# liklihood function... takes 3 potential parameter values, gives predictions (y hat... give it vector of x's, it gives you prediction of y's)... we set
# these predictions as the mean, and then it will center a normal distribution on this predicted value, and then give the hight of density for every single observed value of y
# ...so we end up with 101 densities...then we sum them up to get the total density of getting the observed data from our model, given the parameters
# we fed into it... so when we get the true parameters, this should have the highest density... this is what happens with our graph... we give
# it true B and true SD, so only variable is a.  We then lapply over a sequence of potential A's, and it ends up plotting a distriubtion centered
# over 12.5 (our true slope). In 'reality we wouldn't know the true' B and SD, but this just illustrates what's going on'.
# so d norm is basically giving us how off our model is from the y data.  for each y point, imagine taking the predicted value as the mean of a normal...
# plotting where true y is on this distribution, and then calculating the prob density height of that y.  if it was totally accurate, all of the ys
# would fall on the mean of each distriubtion.  so we end up with 101 probabilities... and then we sum them all up to get the total prob for that parameter set.
# "The likelihood is the probability (density) with which we would expect the observed data to occur conditional on the parameters of the model that we look at. So, given that our linear model y = b + a*x + N(0,sd)
# takes the parameters (a, b, sd) as an input, we have to return the probability of obtaining the test data above under this model"
###################################################################################################################################
#Now specify our priors. Just assume uniform and normal prior (marginal) distriubtions
prior = function(param){
a = param[1]
b = param[2]
sd = param[3]
a_prior = dunif(a, min=0, max=20, log=T)
b_prior = dnorm(b, sd=5, log=T)
sd_prior = dunif(sd, min=0, max=40, log=T)
#since they are logs, we add them (instead of multiply) to get joint prob assuming independence?
return(a_prior + b_prior + sd_prior)
}
####################################################################################################
#Calculate the posterior... here we add again because we are working with logs
posterior = function(param){
return (likelihood(param) + prior(param))
}
################################################################################################
###Metropolosis algorithm
proposalfunction = function(param){
return(rnorm(3, mean=param, sd = c(0, 0.5, 0.3)))
}
run_metropolosis_MCMC = function(startvalue, iterations){
chain = array(dim = c(iterations +1, 3))
chain[1, ] = startvalue
for (i in 1:iterations){
proposal = proposalfunction(chain[i, ])
probab = exp(posterior(proposal) - posterior(chain[i , ]))
if (runif(1) < probab){
chain[i+1, ] = proposal
}else{
chain[i+1, ] = chain[i, ]
}
}
return(chain)
}
startvalue = c(8, 0 , 10)
chain = run_metropolosis_MCMC(startvalue, 10000)
burnIn = 5000
hist(chain[-(1:burnIn), 1], nclass= 50, main="Posterior of A")
abline(v = trueA, col="red")
trueA = 5
trueB = 0
trueSd =10
sampleSize = 101
#create independent x-values
x = (-(sampleSize-1)/2):((sampleSize-1)/2)
#create y values as lineary combo plus error (ax + b + N(0,sd))
y = trueA * x + trueB + rnorm(n = sampleSize, mean= 0, sd = trueSd)
plot(x,y, main="Test Data")
#create liklihood function
#dnorm gives heights of probability distribution at each point with mean and sd specified
#use log here because prob hieghts can get very small, and might run into computer precision errors.
likelihood = function(param){
a = param[1]
b = param[2]
sd = param[3]
pred = a*x + b
singlelikelihoods = dnorm(y, mean = pred, sd = sd, log = TRUE)
sumll = sum(singlelikelihoods)
return(sumll)
}
#example plot to show that it estimates highest prob density over true a parameter if we give it the actual B and SD parameters.
#slopevalues = function(potential_a){return(likelihood(c(potential_a, trueB, trueSd)))}
#slopelikelihoods = lapply(seq(3,7, by=0.05), slopevalues)
#plot (seq(3,7, by = 0.05), slopelikelihoods, type="l", xlab="value of slope paramater a", ylab = "Log likelihood")
# liklihood function... takes 3 potential parameter values, gives predictions (y hat... give it vector of x's, it gives you prediction of y's)... we set
# these predictions as the mean, and then it will center a normal distribution on this predicted value, and then give the hight of density for every single observed value of y
# ...so we end up with 101 densities...then we sum them up to get the total density of getting the observed data from our model, given the parameters
# we fed into it... so when we get the true parameters, this should have the highest density... this is what happens with our graph... we give
# it true B and true SD, so only variable is a.  We then lapply over a sequence of potential A's, and it ends up plotting a distriubtion centered
# over 12.5 (our true slope). In 'reality we wouldn't know the true' B and SD, but this just illustrates what's going on'.
# so d norm is basically giving us how off our model is from the y data.  for each y point, imagine taking the predicted value as the mean of a normal...
# plotting where true y is on this distribution, and then calculating the prob density height of that y.  if it was totally accurate, all of the ys
# would fall on the mean of each distriubtion.  so we end up with 101 probabilities... and then we sum them all up to get the total prob for that parameter set.
# "The likelihood is the probability (density) with which we would expect the observed data to occur conditional on the parameters of the model that we look at. So, given that our linear model y = b + a*x + N(0,sd)
# takes the parameters (a, b, sd) as an input, we have to return the probability of obtaining the test data above under this model"
###################################################################################################################################
#Now specify our priors. Just assume uniform and normal prior (marginal) distriubtions
prior = function(param){
a = param[1]
b = param[2]
sd = param[3]
a_prior = dunif(a, min=0, max=10, log=T)
b_prior = dnorm(b, sd=5, log=T)
sd_prior = dunif(sd, min=0, max=30, log=T)
#since they are logs, we add them (instead of multiply) to get joint prob assuming independence?
return(a_prior + b_prior + sd_prior)
}
####################################################################################################
#Calculate the posterior... here we add again because we are working with logs
posterior = function(param){
return (likelihood(param) + prior(param))
}
################################################################################################
###Metropolosis algorithm
proposalfunction = function(param){
return(rnorm(3, mean=param, sd = c(0.1, 0.5, 0.3)))
}
run_metropolosis_MCMC = function(startvalue, iterations){
chain = array(dim = c(iterations +1, 3))
chain[1, ] = startvalue
for (i in 1:iterations){
proposal = proposalfunction(chain[i, ])
probab = exp(posterior(proposal) - posterior(chain[i , ]))
if (runif(1) < probab){
chain[i+1, ] = proposal
}else{
chain[i+1, ] = chain[i, ]
}
}
return(chain)
}
startvalue = c(4, 0 , 10)
chain = run_metropolosis_MCMC(startvalue, 100000)
burnIn = 5000
hist(chain[-(1:burnIn), 1], nclass= 50, main="Posterior of A")
abline(v = trueA, col="red")
startvalue = c(4, 0 , 10)
chain = run_metropolosis_MCMC(startvalue, 1000000)
burnIn = 50000
hist(chain[-(1:burnIn), 1], nclass= 50, main="Posterior of A")
abline(v = trueA, col="red")
#https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/
trueA = 5
trueB = 0
trueSd =0.2
sampleSize = 101
#create independent x-values
x = (-(sampleSize-1)/2):((sampleSize-1)/2)
#create y values as lineary combo plus error (ax + b + N(0,sd))
y = trueA * x + trueB + rnorm(n = sampleSize, mean= 0, sd = trueSd)
plot(x,y, main="Test Data")
#create liklihood function
#dnorm gives heights of probability distribution at each point with mean and sd specified
#use log here because prob hieghts can get very small, and might run into computer precision errors.
likelihood = function(param){
a = param[1]
b = param[2]
sd = param[3]
pred = a*x + b
singlelikelihoods = dnorm(y, mean = pred, sd = sd, log = TRUE)
sumll = sum(singlelikelihoods)
return(sumll)
}
#example plot to show that it estimates highest prob density over true a parameter if we give it the actual B and SD parameters.
#slopevalues = function(potential_a){return(likelihood(c(potential_a, trueB, trueSd)))}
#slopelikelihoods = lapply(seq(3,7, by=0.05), slopevalues)
#plot (seq(3,7, by = 0.05), slopelikelihoods, type="l", xlab="value of slope paramater a", ylab = "Log likelihood")
# liklihood function... takes 3 potential parameter values, gives predictions (y hat... give it vector of x's, it gives you prediction of y's)... we set
# these predictions as the mean, and then it will center a normal distribution on this predicted value, and then give the hight of density for every single observed value of y
# ...so we end up with 101 densities...then we sum them up to get the total density of getting the observed data from our model, given the parameters
# we fed into it... so when we get the true parameters, this should have the highest density... this is what happens with our graph... we give
# it true B and true SD, so only variable is a.  We then lapply over a sequence of potential A's, and it ends up plotting a distriubtion centered
# over 12.5 (our true slope). In 'reality we wouldn't know the true' B and SD, but this just illustrates what's going on'.
# so d norm is basically giving us how off our model is from the y data.  for each y point, imagine taking the predicted value as the mean of a normal...
# plotting where true y is on this distribution, and then calculating the prob density height of that y.  if it was totally accurate, all of the ys
# would fall on the mean of each distriubtion.  so we end up with 101 probabilities... and then we sum them all up to get the total prob for that parameter set.
# "The likelihood is the probability (density) with which we would expect the observed data to occur conditional on the parameters of the model that we look at. So, given that our linear model y = b + a*x + N(0,sd)
# takes the parameters (a, b, sd) as an input, we have to return the probability of obtaining the test data above under this model"
###################################################################################################################################
#Now specify our priors. Just assume uniform and normal prior (marginal) distriubtions
prior = function(param){
a = param[1]
b = param[2]
sd = param[3]
a_prior = dunif(a, min=0, max=10, log=T)
b_prior = dnorm(b, sd=5, log=T)
sd_prior = dunif(sd, min=0, max=30, log=T)
#since they are logs, we add them (instead of multiply) to get joint prob assuming independence?
return(a_prior + b_prior + sd_prior)
}
####################################################################################################
#Calculate the posterior... here we add again because we are working with logs
posterior = function(param){
return (likelihood(param) + prior(param))
}
################################################################################################
###Metropolosis algorithm
proposalfunction = function(param){
return(rnorm(3, mean=param, sd = c(0.1, 0.5, 0.3)))
}
run_metropolosis_MCMC = function(startvalue, iterations){
chain = array(dim = c(iterations +1, 3))
chain[1, ] = startvalue
for (i in 1:iterations){
proposal = proposalfunction(chain[i, ])
probab = exp(posterior(proposal) - posterior(chain[i , ]))
if (runif(1) < probab){
chain[i+1, ] = proposal
}else{
chain[i+1, ] = chain[i, ]
}
}
return(chain)
}
startvalue = c(4, 0 , 10)
chain = run_metropolosis_MCMC(startvalue, 10000)
burnIn = 5000
hist(chain[-(1:burnIn), 1], nclass= 50, main="Posterior of A")
abline(v = trueA, col="red")
#https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/
trueA = 5
trueB = 0
trueSd =0.2
sampleSize = 101
#create independent x-values
x = (-(sampleSize-1)/2):((sampleSize-1)/2)
#create y values as lineary combo plus error (ax + b + N(0,sd))
y = trueA * x + trueB + rnorm(n = sampleSize, mean= 0, sd = trueSd)
plot(x,y, main="Test Data")
#create liklihood function
#dnorm gives heights of probability distribution at each point with mean and sd specified
#use log here because prob hieghts can get very small, and might run into computer precision errors.
likelihood = function(param){
a = param[1]
b = param[2]
sd = param[3]
pred = a*x + b
singlelikelihoods = dnorm(y, mean = pred, sd = sd, log = TRUE)
sumll = sum(singlelikelihoods)
return(sumll)
}
#example plot to show that it estimates highest prob density over true a parameter if we give it the actual B and SD parameters.
#slopevalues = function(potential_a){return(likelihood(c(potential_a, trueB, trueSd)))}
#slopelikelihoods = lapply(seq(3,7, by=0.05), slopevalues)
#plot (seq(3,7, by = 0.05), slopelikelihoods, type="l", xlab="value of slope paramater a", ylab = "Log likelihood")
# liklihood function... takes 3 potential parameter values, gives predictions (y hat... give it vector of x's, it gives you prediction of y's)... we set
# these predictions as the mean, and then it will center a normal distribution on this predicted value, and then give the hight of density for every single observed value of y
# ...so we end up with 101 densities...then we sum them up to get the total density of getting the observed data from our model, given the parameters
# we fed into it... so when we get the true parameters, this should have the highest density... this is what happens with our graph... we give
# it true B and true SD, so only variable is a.  We then lapply over a sequence of potential A's, and it ends up plotting a distriubtion centered
# over 12.5 (our true slope). In 'reality we wouldn't know the true' B and SD, but this just illustrates what's going on'.
# so d norm is basically giving us how off our model is from the y data.  for each y point, imagine taking the predicted value as the mean of a normal...
# plotting where true y is on this distribution, and then calculating the prob density height of that y.  if it was totally accurate, all of the ys
# would fall on the mean of each distriubtion.  so we end up with 101 probabilities... and then we sum them all up to get the total prob for that parameter set.
# "The likelihood is the probability (density) with which we would expect the observed data to occur conditional on the parameters of the model that we look at. So, given that our linear model y = b + a*x + N(0,sd)
# takes the parameters (a, b, sd) as an input, we have to return the probability of obtaining the test data above under this model"
###################################################################################################################################
#Now specify our priors. Just assume uniform and normal prior (marginal) distriubtions
prior = function(param){
a = param[1]
b = param[2]
sd = param[3]
a_prior = dunif(a, min=0, max=10, log=T)
b_prior = dnorm(b, sd=5, log=T)
sd_prior = dunif(sd, min=0, max=10, log=T)
#since they are logs, we add them (instead of multiply) to get joint prob assuming independence?
return(a_prior + b_prior + sd_prior)
}
####################################################################################################
#Calculate the posterior... here we add again because we are working with logs
posterior = function(param){
return (likelihood(param) + prior(param))
}
################################################################################################
###Metropolosis algorithm
proposalfunction = function(param){
return(rnorm(3, mean=param, sd = c(0.1, 0.5, 0.3)))
}
run_metropolosis_MCMC = function(startvalue, iterations){
chain = array(dim = c(iterations +1, 3))
chain[1, ] = startvalue
for (i in 1:iterations){
proposal = proposalfunction(chain[i, ])
probab = exp(posterior(proposal) - posterior(chain[i , ]))
if (runif(1) < probab){
chain[i+1, ] = proposal
}else{
chain[i+1, ] = chain[i, ]
}
}
return(chain)
}
startvalue = c(4, 0 , 0)
chain = run_metropolosis_MCMC(startvalue, 10000)
burnIn = 5000
hist(chain[-(1:burnIn), 1], nclass= 50, main="Posterior of A")
abline(v = trueA, col="red")
runif(1)
runif(1)
runif(1)
runif(1)
remove(chain)
#https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/
trueA = 5
trueB = 0
trueSd =0.2
sampleSize = 101
#create independent x-values
x = (-(sampleSize-1)/2):((sampleSize-1)/2)
#create y values as lineary combo plus error (ax + b + N(0,sd))
y = trueA * x + trueB + rnorm(n = sampleSize, mean= 0, sd = trueSd)
plot(x,y, main="Test Data")
#create liklihood function
#dnorm gives heights of probability distribution at each point with mean and sd specified
#use log here because prob hieghts can get very small, and might run into computer precision errors.
likelihood = function(param){
a = param[1]
b = param[2]
sd = param[3]
pred = a*x + b
singlelikelihoods = dnorm(y, mean = pred, sd = sd, log = TRUE)
sumll = sum(singlelikelihoods)
return(sumll)
}
#example plot to show that it estimates highest prob density over true a parameter if we give it the actual B and SD parameters.
#slopevalues = function(potential_a){return(likelihood(c(potential_a, trueB, trueSd)))}
#slopelikelihoods = lapply(seq(3,7, by=0.05), slopevalues)
#plot (seq(3,7, by = 0.05), slopelikelihoods, type="l", xlab="value of slope paramater a", ylab = "Log likelihood")
# liklihood function... takes 3 potential parameter values, gives predictions (y hat... give it vector of x's, it gives you prediction of y's)... we set
# these predictions as the mean, and then it will center a normal distribution on this predicted value, and then give the hight of density for every single observed value of y
# ...so we end up with 101 densities...then we sum them up to get the total density of getting the observed data from our model, given the parameters
# we fed into it... so when we get the true parameters, this should have the highest density... this is what happens with our graph... we give
# it true B and true SD, so only variable is a.  We then lapply over a sequence of potential A's, and it ends up plotting a distriubtion centered
# over 12.5 (our true slope). In 'reality we wouldn't know the true' B and SD, but this just illustrates what's going on'.
# so d norm is basically giving us how off our model is from the y data.  for each y point, imagine taking the predicted value as the mean of a normal...
# plotting where true y is on this distribution, and then calculating the prob density height of that y.  if it was totally accurate, all of the ys
# would fall on the mean of each distriubtion.  so we end up with 101 probabilities... and then we sum them all up to get the total prob for that parameter set.
# "The likelihood is the probability (density) with which we would expect the observed data to occur conditional on the parameters of the model that we look at. So, given that our linear model y = b + a*x + N(0,sd)
# takes the parameters (a, b, sd) as an input, we have to return the probability of obtaining the test data above under this model"
###################################################################################################################################
#Now specify our priors. Just assume uniform and normal prior (marginal) distriubtions
prior = function(param){
a = param[1]
b = param[2]
sd = param[3]
a_prior = dunif(a, min=0, max=10, log=T)
b_prior = dnorm(b, sd=5, log=T)
sd_prior = dunif(sd, min=0, max=10, log=T)
#since they are logs, we add them (instead of multiply) to get joint prob assuming independence?
return(a_prior + b_prior + sd_prior)
}
####################################################################################################
#Calculate the posterior... here we add again because we are working with logs
posterior = function(param){
return (likelihood(param) + prior(param))
}
################################################################################################
###Metropolosis algorithm
proposalfunction = function(param){
return(rnorm(3, mean=param, sd = c(0.1, 0.5, 0.3)))
}
run_metropolosis_MCMC = function(startvalue, iterations){
chain = array(dim = c(iterations +1, 3))
chain[1, ] = startvalue
for (i in 1:iterations){
proposal = proposalfunction(chain[i, ])
probab = exp(posterior(proposal) - posterior(chain[i , ]))
if (runif(1) < probab){
chain[i+1, ] = proposal
}else{
chain[i+1, ] = chain[i, ]
}
}
return(chain)
}
startvalue = c(0, 0 , 0)
chain = run_metropolosis_MCMC(startvalue, 10000)
burnIn = 5000
hist(chain[-(1:burnIn), 1], nclass= 50, main="Posterior of A")
abline(v = trueA, col="red")
.16/(.16+.03+.17)
.16/.36
10^6
92*0.5
+5
92*0.5+5
49/.45
0.45*x=39
39/0.45
load("C:/Users/frankerrickson/Desktop/fortran_30_2.txt")
fortran_30 <- read.table("C:/Users/frankerrickson/Desktop/fortran_30.txt", quote="\"", comment.char="")
View(fortran_30)
fortran_30_2 <- read.table("C:/Users/frankerrickson/Desktop/fortran_30_2.txt", quote="\"", comment.char="")
View(fortran_30_2)
fortran_30_2=fortran_30_2[,c(1,3,5,7)]
fortran_30 <- read.table("C:/Users/frankerrickson/Desktop/fortran_30.txt", quote="\"", comment.char="")
View(fortran_30)
colnames(fortran_30) = c("timestep", "DTE1", "DTE2", "temp_landair", "temp_sst", "QL", "Q0", "DelQL", "QC", "DQ", "DPAST(1)", "DPAST(2)", "DTEAUX")
View(fortran_30)
colnames(fortran_30) = c("timestep", "DTE1", "DTE2", "temp_landair", "temp_sst", "QL", "Q0", "DelQL", "DelQ0", "QC", "DQ", "DPAST(1)", "DPAST(2)", "DTEAUX")
View(fortran_30)
View(fortran_30_2)
colnames(fortran_30_2) = c("forcing", "temp", "heatflux_mixed", "heatflux_interior")
View(fortran_30_2)
ALL_fortran = cbind(fortran_30, fortran_30_2)
View(ALL_fortran)
fortran_30 <- read.table("C:/Users/frankerrickson/Desktop/fortran_30.txt", quote="\"", comment.char="")
View(fortran_30)
View(ALL_fortran)
fortran_30 <- read.table("C:/Users/frankerrickson/Desktop/fortran_30.txt", quote="\"", comment.char="")
View(fortran_30)
colnames(fortran_30) = c("timestep", "DTE1", "DTE2", "temp_landair", "temp_sst", "QL", "Q0", "DelQL", "DelQ0", "QC1", "QC2", "DQ1", "DQ2", "DPAST(1)", "DPAST(2)")
View(fortran_30)
ALL_fortran = cbind(fortran_30, fortran_30_2)
View(ALL_fortran)
rep(NA, times = length(4))
bone = c(1,2,3,4)
rep(NA, times = length(bone))
Square_stuff = function(data) {
dim = length(data)
Values = rep(NA, times = dim)
for (i in 1:dim)
Values[i] = x[i]^2
return(Values)
}
data=c(1,2,3,4)
Square_stuff(1,2,3,4)
Square_stuff(data)
Square_stuff = function(data) {
dim = length(data)
Values = rep(NA, times = dim)
for (i in 1:dim)
Values[i] = data[i]^2
return(Values)
}
Square_stuff(data)
Square_stuff_specific = function(data, index) {
dim = length(data)
Values = rep(NA, times = dim)
for (i in 1:dim)
Values[i] = data[i]^2
return(Values[index])
}
Square_stuff_specific(data,3)
assim.temp = TRUE
assim.ocheat = TRUE
assim.co2inst = TRUE
assim.co2ice = TRUE
assim.ocflux = TRUE
assim.moc = FALSE
endyear = 2010
source("sneasy.R") # SNEASY Earth system model
source("loadobs.R") # load observations
source("assim.R") # Bayesian parameter estimation
setup.sneasy() # call th
setwd("C:/Users/frankerrickson/Desktop/Masters_Project/Code/sneasy/R")
assim.temp = TRUE
assim.ocheat = TRUE
assim.co2inst = TRUE
assim.co2ice = TRUE
assim.ocflux = TRUE
assim.moc = FALSE
endyear = 2010
source("sneasy.R") # SNEASY Earth system model
source("loadobs.R") # load observations
source("assim.R") # Bayesian parameter estimation
setup.sneasy() # call th
p0 = c(2.7,2.9,1.0,4.2,0.9,23,0.03,-0.06,-33,286,19.5,0.1,2.0,0.45,2.25,0.55,0.9,0.95)
log.post(p0)
